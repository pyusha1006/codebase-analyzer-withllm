# LLM Configuration - Using Ollama (FREE, runs locally)
LLM_PROVIDER=ollama

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=codellama

# Analysis Configuration
MAX_TOKENS_PER_CHUNK=8000
CODEBASE_PATH=../SakilaProject
TARGET_LANGUAGE=auto
OUTPUT_PATH=./output/analysis_results.json
